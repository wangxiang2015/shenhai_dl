
model:
  name: 'transformer'
  pretrained: false
  d_model: 256   # embedding size
  nhead: 8       # number of heads
  d_ff: 2048     # feed forward layer size
  num_layers: 8  # number of encoding layers
  dropout_rate: 0.2
  deepfeat_sz: 64
  nb_demo: 2
  nb_feats: 20
  model_dir: './ckpt_dir'

training:
  batch_size: 128
  learning_rate: 0.001
  max_epochs: 3 # default: 200
  log_dir: './log_dir'



  